{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5919391",
   "metadata": {
    "id": "mUG3S1JrYfrZ"
   },
   "source": [
    "<p style=\"text-align: center\"><img src=\"https://gitlab.aicrowd.com/aicrowd/assets/-/raw/master/challenges/clock-decomposition/notebook-banner.jpg?inline=false\" alt=\"Drawing\" style=\"height: 400px;\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0fb24f",
   "metadata": {
    "id": "7Uc5MnPWYfrk",
    "tags": []
   },
   "source": [
    "# Install packages üóÉ\n",
    "\n",
    "Please add all pacakage installations in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075b320",
   "metadata": {
    "id": "dp9ihAHyYfrk",
    "outputId": "b9ae1efc-f354-45a6-f482-b85c0316cc45"
   },
   "outputs": [],
   "source": [
    "!pip install numpy pandas lightgbm optuna scikit-Optimize imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7d606",
   "metadata": {
    "id": "M8tWSe6BYfrl",
    "tags": []
   },
   "source": [
    "# Define preprocessing code üíª\n",
    "\n",
    "The code that is common between the training and the prediction sections should be defined here. During evaluation, we completely skip the training section. Please make sure to add any common logic between the training and prediction sections here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04f43d",
   "metadata": {
    "id": "aRiRGZytYfrl"
   },
   "source": [
    "## Import common packages\n",
    "\n",
    "Import packages that are common for training and prediction phases here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29145aad",
   "metadata": {
    "id": "22H5Tt4sYfrm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f6aeb",
   "metadata": {},
   "source": [
    "## Fill in missing values\n",
    "We define a function to fill in the missing values with different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091fb59b",
   "metadata": {
    "id": "VCNiVc1CYfrm"
   },
   "outputs": [],
   "source": [
    "# Define the function that fill in missing Value\n",
    "def fillin_na(df):\n",
    "    # fill in NA in column starts with missing digit with 1\n",
    "    df.update(df.filter(regex='missing_digit', axis=1).fillna(1))\n",
    "\n",
    "    # fill in NA in columns dist from cen & euc_dist_digit with mean\n",
    "    for i in range(0, 12):\n",
    "        df[str(i+1) + ' dist from cen'].fillna(df[str(i+1) +\n",
    "                                                  ' dist from cen'].mean(), inplace=True)\n",
    "        df['euc_dist_digit_' +\n",
    "            str(i+1)].fillna(df['euc_dist_digit_' + str(i+1)].mean(), inplace=True)\n",
    "\n",
    "    # fill in NA in column starts with area_digit / height_digit / width_digit with 0\n",
    "    df.update(\n",
    "        df.filter(regex=r'(area_digit|height_digit|width_digit)', axis=1).fillna(0))\n",
    "\n",
    "    # fill value with -999\n",
    "    ex_neg_col = ['variance_width', 'variance_height', 'variance_area', 'deviation_dist_from_mid_axis',\n",
    "                  'between_axis_digits_angle_sum', 'between_axis_digits_angle_var', 'time_diff', 'centre_dot_detect']\n",
    "    df[ex_neg_col] = df[ex_neg_col].fillna(-999)\n",
    "\n",
    "    drop_col = ['between_digits_angle_ccw_sum', 'between_digits_angle_cw_sum',\n",
    "                'between_digits_angle_ccw_var', 'sequence_flag_ccw', 'actual_hour_digit', 'actual_minute_digit']\n",
    "    df[drop_col] = df[drop_col].fillna(-999)\n",
    "\n",
    "    # fill value with 0\n",
    "    fil_with_zero = ['number_of_digits', 'number_of_hands', 'hand_count_dummy',\n",
    "                     'hour_hand_length', 'minute_hand_length', 'single_hand_length', 'clockhand_ratio', 'clockhand_diff',\n",
    "                     'angle_between_hands', 'double_major', 'double_minor', 'vertical_dist', 'horizontal_dist']\n",
    "    df[fil_with_zero] = df[fil_with_zero].fillna(0)\n",
    "\n",
    "    # fill value with -1\n",
    "    fil_with_one = ['deviation_from_centre', 'hour_proximity_from_11', 'minute_proximity_from_2',\n",
    "                    'hour_pointing_digit', 'minute_pointing_digit', 'final_rotation_angle',\n",
    "                    'top_area_perc', 'bottom_area_perc', 'left_area_perc', 'right_area_perc',\n",
    "                    'between_digits_angle_cw_var', 'ellipse_circle_ratio', 'sequence_flag_cw', 'percentage_inside_ellipse']\n",
    "    df[fil_with_one] = df[fil_with_one].fillna(-1)\n",
    "\n",
    "    # Add missing value indicator\n",
    "    df['time_diff_ind'] = df[\"time_diff\"].apply(\n",
    "        lambda x: 1 if math.isnan(x) else 0)\n",
    "    df['centre_dot_detect_ind'] = df[\"centre_dot_detect\"].apply(\n",
    "        lambda x: 1 if math.isnan(x) else 0)\n",
    "\n",
    "    # fill missing categorical variables\n",
    "    df['intersection_pos_rel_centre'] = df['intersection_pos_rel_centre'].fillna(\n",
    "        'Others')\n",
    "    # OneHotEoncoding Categorical Variables\n",
    "    df = pd.get_dummies(data=df, prefix=['intersec_dire'], columns=[\n",
    "                        'intersection_pos_rel_centre'], drop_first=True)\n",
    "\n",
    "    # drop unnecessary variables\n",
    "    drop_col = ['between_digits_angle_ccw_sum', 'between_digits_angle_cw_sum',\n",
    "                'between_digits_angle_ccw_var', 'sequence_flag_ccw', 'actual_hour_digit', 'actual_minute_digit']\n",
    "    df = df.drop(drop_col, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0bbba",
   "metadata": {
    "id": "mI2-YaJ5Yfrn",
    "tags": []
   },
   "source": [
    "# Training phase ‚öôÔ∏è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008cbd28",
   "metadata": {
    "id": "BoNJpSwoYfro"
   },
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9667f6f",
   "metadata": {
    "id": "aJkL4KiiYfro"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(AICROWD_DATASET_PATH.replace(\"validation\", \"train\"))\n",
    "# Fill in the NA\n",
    "train = fillin_na(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec537f0b",
   "metadata": {
    "id": "QDP1nN5QYfro",
    "outputId": "a4266b67-9fbd-4978-ecb5-4593d6fbe3f5"
   },
   "outputs": [],
   "source": [
    "train['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc396496",
   "metadata": {},
   "source": [
    "## Preprocessing - balance the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4493f461",
   "metadata": {},
   "source": [
    "### Simple undersample with random drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7143564",
   "metadata": {
    "id": "H6SO8wKNYfrp"
   },
   "outputs": [],
   "source": [
    "# Pre-process the training data\n",
    "\n",
    "# Simple Undersampling\n",
    "train_us = pd.concat([\n",
    "    train.loc[train.diagnosis == 'pre_alzheimer'],\n",
    "    train.loc[train.diagnosis == 'post_alzheimer'],\n",
    "    train.loc[train.diagnosis == 'normal'].sample(frac=1/6),\n",
    "]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Seperate the target variable\n",
    "train_y = train_us[['diagnosis']].values.ravel()\n",
    "\n",
    "# Label encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "train_y = label_encoder.fit_transform(np.array(train_y))\n",
    "\n",
    "# Get the no cat data\n",
    "cat_col = ['row_id', 'diagnosis']\n",
    "train_no_cat = train_us.drop(cat_col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be0d3d8",
   "metadata": {},
   "source": [
    "### Oversample with SMOTE and undersample with random drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa9e4d",
   "metadata": {
    "id": "-InUicX3Yfrp",
    "outputId": "5badbece-9484-4557-8173-6d6cafcb0c23"
   },
   "outputs": [],
   "source": [
    "''' Not good as Random Drop\n",
    "# SMOTE\n",
    "cat_col = ['row_id','diagnosis']\n",
    "train_no_cat = train.drop(cat_col,axis=1)\n",
    "train_y = train['diagnosis']\n",
    "\n",
    "over = SMOTE(sampling_strategy={\n",
    "    'normal':31208,\n",
    "    'post_alzheimer':1149*5,\n",
    "    'pre_alzheimer':420*5\n",
    "})\n",
    "under = RandomUnderSampler(sampling_strategy={\n",
    "    'normal':int(31208/4), # keeping 25%\n",
    "    'post_alzheimer':1149*5, # Keeping all of the samples we generated in the previous step\n",
    "    'pre_alzheimer':420*5\n",
    "})\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "train_no_cat, train_y = pipeline.fit_resample(train_no_cat, train_y)\n",
    "\n",
    "train_y = train_y.values.ravel()\n",
    "\n",
    "# Label encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "train_y = label_encoder.fit_transform(np.array(train_y))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e1f53",
   "metadata": {
    "id": "SbNIKwDgYfrq"
   },
   "source": [
    "## Feature Importantce Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2a768",
   "metadata": {
    "id": "6yObOo33Yfrq",
    "outputId": "3b2d722e-1360-43f3-cf00-c979153e2a83"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "model = XGBClassifier()\n",
    "fs = SelectFromModel(model)\n",
    "fs.fit(X_train, y_train)\n",
    "# X_train_fs = fs.transform(X_train)\n",
    "\n",
    "feature_idx = fs.get_support()\n",
    "feature_name = X_train.columns[feature_idx]\n",
    "\n",
    "print(feature_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1a17f",
   "metadata": {
    "id": "z5iIu4DKYfrr"
   },
   "source": [
    "## Hyper-parameter Tuning - LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a5a07",
   "metadata": {
    "id": "8_ytubKMYfrr",
    "outputId": "9a675659-854b-4847-9ab2-dab77df9d4ca"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "   \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train_no_cat,train_y, test_size=0.2)\n",
    "    \n",
    "    \n",
    "    param = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        'num_class': 3, \n",
    "        \"metric\": \"multi_logloss\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 64, 256),\n",
    "        'min_data_in_leaf': trial.suggest_int(\"min_data_in_leaf\", 20, 100),  \n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        'is_unbalance': True,\n",
    "        'verbosity': -1\n",
    "    }\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval  = lgb.Dataset(X_valid, y_valid, reference = lgb_train)\n",
    "    model_lgb = lgb.train(param, lgb_train, 500, valid_sets=[lgb_eval], \n",
    "                    early_stopping_rounds=100)\n",
    "    \n",
    "    pred = model_lgb.predict(X_valid,num_iteration = model_lgb.best_iteration)\n",
    "        \n",
    "    logloss_lgb = log_loss(y_valid,pred)\n",
    "\n",
    "    return logloss_lgb\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c0cd0",
   "metadata": {
    "id": "DHASfBJjYfru"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1fb5c9",
   "metadata": {
    "id": "WxcxKzu1Yfru"
   },
   "outputs": [],
   "source": [
    "# K- fold\n",
    "X_train = train_no_cat\n",
    "y_train = pd.DataFrame(train_y)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2021, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9a20c",
   "metadata": {
    "id": "CpDH_8pSYfrv",
    "outputId": "03da7580-67a6-4645-8eca-970c7bcb7724"
   },
   "outputs": [],
   "source": [
    "model_lgbs = []\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_logloss',\n",
    "    'num_leaves': 128,\n",
    "    'min_data_in_leaf': 100,\n",
    "    'learning_rate': 0.04,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_gain_to_split': 0.2,\n",
    "    'is_unbalance': True,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "for fold, (itrain, ivalid) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Running for fold {fold}\")\n",
    "    lgb_train = lgb.Dataset(X_train.iloc[itrain], y_train.iloc[itrain])\n",
    "    lgb_eval = lgb.Dataset(\n",
    "        X_train.iloc[ivalid], y_train.iloc[ivalid], reference=lgb_train)\n",
    "    model_lgb = lgb.train(params, lgb_train, 500, valid_sets=[lgb_eval],\n",
    "                          early_stopping_rounds=100)\n",
    "\n",
    "    model_lgbs.append(model_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d84306",
   "metadata": {
    "id": "-8dHL0YYYfrv"
   },
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab371c",
   "metadata": {
    "id": "JGJkhZsIYfrv",
    "outputId": "5c175803-95fc-4d1c-a7f7-58acfa11b8b0"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model_cats = []\n",
    "\n",
    "for fold, (itrain, ivalid) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Running for fold {fold}\")\n",
    "    model_cat = CatBoostClassifier()\n",
    "    model_cat.fit(X_train.iloc[itrain], y_train.iloc[itrain].values.ravel())\n",
    "    model_cats.append(model_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc4640",
   "metadata": {
    "id": "y9tUujjpYfrx"
   },
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a7776",
   "metadata": {
    "id": "2eyi_UfiYfrx"
   },
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "valid_data = pd.read_csv(AICROWD_DATASET_PATH)\n",
    "valid_data = fillin_na(valid_data)\n",
    "valid_no_cat = valid_data.drop(['row_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7fa33",
   "metadata": {
    "id": "rTENYLvuYfrx"
   },
   "outputs": [],
   "source": [
    "# Load validation labels\n",
    "valid_y = pd.read_csv(AICROWD_DATASET_PATH.replace(\n",
    "    \"validation\", \"validation_ground_truth\"))\n",
    "valid_y = valid_y['diagnosis']\n",
    "nb_folds = 5  # skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d0236",
   "metadata": {
    "id": "CfgcNiyqYfry",
    "outputId": "e992691d-d785-464f-af36-9a96827e4bbb"
   },
   "outputs": [],
   "source": [
    "# lgb\n",
    "lgb_preds = 0.0\n",
    "\n",
    "for fold, model_lgb in enumerate(model_lgbs):\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Running for fold {fold}\")\n",
    "    pred = model_lgb.predict(\n",
    "        valid_no_cat, num_iteration=model_lgb.best_iteration)\n",
    "    lgb_preds += pred/nb_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc2083",
   "metadata": {
    "id": "PBisaVS2Yfry",
    "outputId": "45ccffd8-6dfe-4434-95e5-8057585f6215"
   },
   "outputs": [],
   "source": [
    "# catBoost\n",
    "\n",
    "cat_preds = 0.0\n",
    "\n",
    "for fold, model_cat in enumerate(model_cats):\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Running for fold {fold}\")\n",
    "    pred = model_cat.predict_proba(valid_no_cat)\n",
    "    cat_preds += pred/nb_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecad0f0",
   "metadata": {
    "id": "yvDPRYqlYfry",
    "outputId": "2499708c-d475-4243-e7f5-1914dd327dd1"
   },
   "outputs": [],
   "source": [
    "logloss_lgb = log_loss(valid_y, lgb_preds)\n",
    "logloss_cat = log_loss(valid_y, cat_preds)\n",
    "\n",
    "print(\"LGB logloss: \" + str(logloss_lgb))\n",
    "print(\"Catboost logloss: \" + str(logloss_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97a4d6",
   "metadata": {
    "id": "QljSfTLYYfry"
   },
   "source": [
    "## Save the trained model\n",
    "We decided to proceed with LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5ccd1",
   "metadata": {
    "id": "1wT6E3DCYfrz"
   },
   "outputs": [],
   "source": [
    "# # Save model\n",
    "for i, model_lgb in enumerate(model_lgbs):\n",
    "    model_filename = f'{AICROWD_ASSETS_DIR}/model_lgb_fold_{i}.pkl'\n",
    "    joblib.dump(model_lgb, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ea699",
   "metadata": {
    "id": "k4R40sfAYfrz",
    "tags": []
   },
   "source": [
    "# Prediction phase üîé\n",
    "\n",
    "Please make sure to save the weights from the training section in your assets directory and load them in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe6896",
   "metadata": {
    "id": "gxAriQrfYfrz",
    "outputId": "d001efed-eb98-4917-9d90-8b465cd426ae"
   },
   "outputs": [],
   "source": [
    "nb_folds = 5  # skf.n_splits\n",
    "clfs = []\n",
    "\n",
    "for fold in range(nb_folds):\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Running for fold {fold}\")\n",
    "    model_filename = f'{AICROWD_ASSETS_DIR}/model_lgb_fold_{fold}.pkl'\n",
    "\n",
    "    clf = joblib.load(model_filename)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07791683",
   "metadata": {
    "id": "Aa6PNEyXYfrz"
   },
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29f84f",
   "metadata": {
    "id": "KAdAYAmuYfrz"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(AICROWD_DATASET_PATH)\n",
    "test_data = fillin_na(test_data)\n",
    "test_no_cat = test_data.drop(['row_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e26853",
   "metadata": {
    "id": "t-S8itYGYfrz"
   },
   "source": [
    "## Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ad2dc",
   "metadata": {
    "id": "mhQ4cX4lYfr0",
    "outputId": "d5127f79-a31a-4463-e56e-efcf56c412ab"
   },
   "outputs": [],
   "source": [
    "preds = 0.0\n",
    "for fold, clf in enumerate(clfs):\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Running for fold {fold}\")\n",
    "    pred = clf.predict(test_no_cat, num_iteration=clf.best_iteration)\n",
    "    preds += pred/nb_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b548d8e",
   "metadata": {
    "id": "B0TgiQ52Yfr0"
   },
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    \"row_id\": test_data[\"row_id\"].values,\n",
    "    \"normal_diagnosis_probability\": preds[:, 0],\n",
    "    \"post_alzheimer_diagnosis_probability\": preds[:, 1],\n",
    "    \"pre_alzheimer_diagnosis_probability\": preds[:, 2],\n",
    "}\n",
    "\n",
    "predictions_df = pd.DataFrame.from_dict(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7af680",
   "metadata": {
    "id": "MwqKqvOnYfr0"
   },
   "source": [
    "## Save predictions üì®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a737565",
   "metadata": {
    "id": "56yeB1gPYfr0"
   },
   "outputs": [],
   "source": [
    "predictions_df.to_csv(AICROWD_PREDICTIONS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b407770b",
   "metadata": {
    "id": "9hE11WbPYfr0",
    "tags": []
   },
   "source": [
    "# Submit to AIcrowd üöÄ"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ADDI Alzheimers Detection",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
